---
title: 爬虫入门
date: 2025-02-09T10:54:27.000Z
tags: [crawler]
category: 教程
comments: true
draft: false
---

## 爬虫~~速成~~（入门）

爬虫就是一个访问网页的自动化脚本，分为爬取数据，解析数据，（保存数据），跟随链接，爬取数据可以用requests，selenium，playwright等，解析数据就使用ajax,xpath,json,beautifulsoup等方法

是否允许爬虫需要看大网页的robot.txt，上面会给出界限（防君子不防小人的东西罢了。。。

### HTTP协议min版

#### 概念

HTTP协议中文是超文本传输协议，用于从服务器传输超文本到本地浏览器的传送协议，传递数据包括HTML文件，图片文件(png)，查询结果等

用于服务端-客户端架构，浏览器作为http客户端通过URL向服务端发送请求，服务器收到请求后响应

#### URL

URL：（Uniform Resource Locator）实在互联网上用来标识某一个资源的地址

一个完整的URL包含：

- 协议：http：，之后//作为分隔符
- 域名：例如www.baidu.com,当然io-wy.github.io也开始可以的
- 端口：域名后：作为分隔符，端口可以省略，直接默认端口即可
- 虚拟目录：在两个“ / ”之间的就是虚拟目录，同样可以省略

其余文件名，锚等部分可以先跳过，主要组成还是协议，主机的IP地址（端口号），主机资源里面的具体的地址（目录，文件名等）

#### Request

客户端发送一个HTTP请求到服务器包含四个格式：

请求行，请求头，空行，请求数据

请求方法有多种，如GET,POST,PUT,DELETE等

参考https://zh.wikipedia.org/wiki/%E8%B6%85%E6%96%87%E6%9C%AC%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AE#%E8%AF%B7%E6%B1%82%E6%96%B9%E6%B3%95

这里着重看请求头，包含了浏览器、用户代理、内容类型等信息

![image-20250208180213019](C:\Users\86156\AppData\Roaming\Typora\typora-user-images\image-20250208180213019.png)

#### Response

服务器接受并处理客户端的请求后返回一个HTTP的响应消息包含：

状态行，响应头，空行，响应正文

状态行表示HTTP版本，状态码（如200），状态信息（ok）

响应头有Date:Sat,08 Feb 2025 12:29:13 GMT Content-Yype:text/html

状态码由三个数字组成，第一个数组就是响应类别，比如我们常见的404和200：200 ok ; 400 Bad Request(客户端存在语法错误)；404 Not Found（请求资源不存在）；500 Internal Server Error(服务器出错)，4。。就是客户端错误，5。。就是服务端错误

所以我们常见的步骤就是：

- 客户端连接到Web服务器，
- 发送HTTP请求，
- 服务器接受请求并返回HTTP响应，
- 服务器释放连接TCP连接，（TCP协议用于应用程序之间的通信）
- 客户端浏览器解析HTML的内容

### Python&Request

对于服务端渲染，我们只需要对html进行解析即可，相对于客户端渲染难度小很多

#### 安装

```shell
pip install requests
```

#### 基本使用

以’‘Get’‘请求为例

```python
import requests
url='http://www.baidu.com'
r=requests.get(url)
print('查看结果类型：'，type(r))
print('状态码：'，r.status_code)
print('编码：'，r.encoding)
print('响应头：'，r.headers)
print('查看网络内容：'，r.text)
```

![image-20250208210922751](C:\Users\86156\AppData\Roaming\Typora\typora-user-images\image-20250208210922751.png)

```python
r=requests.post(url)
r=requests.head(url)
r=requests.delete(url)
```

参数有这些get(url, params=None, \*\*kwargs):

具体请看文档：官网https://requests.readthedocs.io/en/latest/api/

#### 具体使用

最开始当然是学校校园网的基本登录

```python
import requests
import time
def set_config():
    Bid=str(input("学号："))
    sever=int(input("运营商："))
    password=str(input("密码"))
    with open("config.bin",wb) as file:
        file.write(str(config).encode("utf-8"))
        file.close()
    print("已保存，准备登录~")
    return config
print("登陆ing")
...
url = ("https://p.njupt.edu.cn:802/eportal/portal/login?callback=dr1003&"
           "login_method=1&"
           "user_account=%2C0%2C{}&"
           "user_password={}&".format(config[0], config[2]))
headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36 Edg/118.0.2088.46",
    "Accept": "*/*"}
r=request.get(url,headers=headers)
if(r.text=='dr1003({"result":1,"msg":"Portal协议认证成功！"});'):
    print("登陆成功")
    break
print("程序退出~~~")
time.sleep(5)
```

大部分代码并不难，text只要去核实具体页面就可以了，如果登录失败也只需要根据具体情况另外开一个elif即可，url和headers是怎么来的呢？

- 随即找一个页面鼠标右键，最下方检查/查看网页源代码
- 选择network然后ctrl+r刷新
- 点击最上方的状态栏，出现headers拉到最下方就有user-agent了

至于为什么要发送带headers的请求呢，因为我们要伪装，假装我们是正常人而不是自动化脚本，

当然不使用headers或许也可以进去？

至于url携带参数可以直接用上面的方法，当然也可以用params携带参数字典，内容的上面官网有

更进一步参考这篇文章https://www.cnblogs.com/hahaha111122222/p/10276583.html

### python& scrapy

scrapy是极其好用的框架！

### python&beautifulsoup

解析静态页面的数据一般是配合requests使用

### python&selenium

客户端渲染比较麻烦，服务端提供框架，结合ajax接口形成完整的页面，用selenium模拟网页动作，直接获得数据是个好办法~

（逆向分析网页我一点不会~，好像是要手动分析network的ajax请求，然后发post请求拿到json数据....）

### pathon$playwright
